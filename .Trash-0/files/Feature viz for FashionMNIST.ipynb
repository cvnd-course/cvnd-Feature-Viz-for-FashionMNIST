{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing CNN Layers\n",
    "---\n",
    "In this notebook, we load a trained CNN (from a solution to FashionMNIST) and implement several feature visualization techniques to see what features this network has learned to extract."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the [data](http://pytorch.org/docs/master/torchvision/datasets.html)\n",
    "\n",
    "In this cell, we load in just the **test** dataset from the FashionMNIST class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# our basic libraries\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "# data loading and transforming\n",
    "from torchvision.datasets import FashionMNIST\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "# The output of torchvision datasets are PILImage images of range [0, 1]. \n",
    "# We transform them to Tensors for input into a CNN\n",
    "\n",
    "## Define a transform to read the data in as a tensor\n",
    "data_transform = transforms.ToTensor()\n",
    "\n",
    "test_data = FashionMNIST(root='./data', train=False,\n",
    "                                  download=True, transform=data_transform)\n",
    "\n",
    "\n",
    "# Print out some stats about the test data\n",
    "print('Test data, number of images: ', len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data loaders, set the batch_size\n",
    "## TODO: you can try changing the batch_size to be larger or smaller\n",
    "## when you get to training your network, see how batch_size affects the loss\n",
    "batch_size = 20\n",
    "\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# specify the image classes\n",
    "classes = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', \n",
    "           'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize some test data\n",
    "\n",
    "This cell iterates over the training dataset, loading a random batch of image/label data, using `dataiter.next()`. It then plots the batch of images and labels in a `2 x batch_size/2` grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "    \n",
    "# obtain one batch of training images\n",
    "dataiter = iter(test_loader)\n",
    "images, labels = dataiter.next()\n",
    "images = images.numpy()\n",
    "\n",
    "# plot the images in the batch, along with the corresponding labels\n",
    "fig = plt.figure(figsize=(25, 4))\n",
    "for idx in np.arange(batch_size):\n",
    "    ax = fig.add_subplot(2, batch_size/2, idx+1, xticks=[], yticks=[])\n",
    "    ax.imshow(np.squeeze(images[idx]), cmap='gray')\n",
    "    ax.set_title(classes[labels[idx]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the network architecture\n",
    "\n",
    "The various layers that make up any neural network are documented, [here](http://pytorch.org/docs/master/nn.html). For a convolutional neural network, we'll use a simple series of layers:\n",
    "* Convolutional layers\n",
    "* Maxpooling layers\n",
    "* Fully-connected (linear) layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        # 1 input image channel (grayscale), 10 output channels/feature maps\n",
    "        # 3x3 square convolution kernel\n",
    "        self.conv1 = nn.Conv2d(1, 10, 3)\n",
    "        \n",
    "        # maxpool layer\n",
    "        # pool with kernel_size=2, stride=2\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        # second conv layer: 10 inputs, 20 outputs, 3x3 conv\n",
    "        self.conv2 = nn.Conv2d(10, 20, 5)\n",
    "        \n",
    "        # 20 outputs * 4*4 to account for the downsampled image size\n",
    "        self.fc1 = nn.Linear(20*4*4, 50)\n",
    "        \n",
    "        # dropout with p=0.5\n",
    "        self.fc1_drop = nn.Dropout(p=0.5)\n",
    "        \n",
    "        # finally, create 10 output channels (for the 10 classes)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    # define the feedforward behavior\n",
    "    def forward(self, x):\n",
    "        # two conv/relu + pool layers\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "\n",
    "        # prep for linear layer\n",
    "        # this line of code is the equivalent of Flatten in Keras\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # two linear layers with dropout in between\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc1_drop(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        # final output\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in our trained net\n",
    "\n",
    "This notebook needs to know the network architecture, as defined above, and once it knows what the \"Net\" class looks like, we can instantiate a model and load in an already trained network.\n",
    "\n",
    "This network is in the directory `saved_models/`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate your Net\n",
    "net = Net()\n",
    "\n",
    "# load the net parameters by name\n",
    "net.load_state_dict(torch.load('saved_models/fashion_net_1.pt'))\n",
    "\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Visualization\n",
    "\n",
    "Sometimes, neural networks are thought of as a black box, given some input, they learn to produce some output. CNN's are actually learning to recognize a variety of spatial patterns and you can visualize what each convolutional layer has been trained to recognize by looking at the weights that make up each convolutional kernel and applying those one at a time to a sample image. These techniques are called feature visualization and they are useful for understanding the inner workings of a CNN.\n",
    "\n",
    "In the cell below, you'll see how to extract and visualize the filter weights for all of the filters in the first convolutional layer.\n",
    "\n",
    "Note the patterns of light and dark pixels and see if you can tell what a particular filter is detecting. For example, the filter pictured in the example below has dark pixels on either side and light pixels in the middle column, and so it may be detecting vertical edges.\n",
    "\n",
    "<img src='edge_filter_ex.png' width= 30% height=30%/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the weights in the first conv layer\n",
    "weights = net.conv1.weight.data\n",
    "w = weights.numpy()\n",
    "\n",
    "# for 10 filters\n",
    "fig=plt.figure(figsize=(20, 8))\n",
    "columns = 5\n",
    "rows = 2\n",
    "for i in range(0, columns*rows):\n",
    "    fig.add_subplot(rows, columns, i+1)\n",
    "    plt.imshow(w[i][0], cmap='gray')\n",
    "    \n",
    "print('First convolutional layer')\n",
    "plt.show()\n",
    "\n",
    "weights = net.conv2.weight.data\n",
    "w = weights.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Maps\n",
    "\n",
    "Next, you'll see how to use OpenCV's `filter2D` function to apply these filters to a sample test image and produce a series of **activation maps** as a result. We'll do this for the first and second convolutional layers and these activation maps whould really give you a sense for what features each filter learns to extract."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain one batch of testing images\n",
    "dataiter = iter(test_loader)\n",
    "images, labels = dataiter.next()\n",
    "images = images.numpy()\n",
    "\n",
    "# select an image by index\n",
    "idx = 2\n",
    "img = np.squeeze(images[idx])\n",
    "\n",
    "# Use OpenCV's filter2D function \n",
    "# apply a specific set of filter weights (like the one's displayed above) to the test image\n",
    "\n",
    "import cv2\n",
    "plt.imshow(img, cmap='gray')\n",
    "\n",
    "weights = net.conv1.weight.data\n",
    "w = weights.numpy()\n",
    "\n",
    "# 1. first conv layer\n",
    "# for 10 filters\n",
    "fig=plt.figure(figsize=(30, 10))\n",
    "columns = 5*2\n",
    "rows = 2\n",
    "for i in range(0, columns*rows):\n",
    "    fig.add_subplot(rows, columns, i+1)\n",
    "    if ((i%2)==0):\n",
    "        plt.imshow(w[int(i/2)][0], cmap='gray')\n",
    "    else:\n",
    "        c = cv2.filter2D(img, -1, w[int((i-1)/2)][0])\n",
    "        plt.imshow(c, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same process but for the second conv layer (20, 5x5 filters):\n",
    "plt.imshow(img, cmap='gray')\n",
    "\n",
    "# second conv layer, conv2\n",
    "weights = net.conv2.weight.data\n",
    "w = weights.numpy()\n",
    "\n",
    "# 1. first conv layer\n",
    "# for 20 filters\n",
    "fig=plt.figure(figsize=(30, 10))\n",
    "columns = 5*2\n",
    "rows = 2*2\n",
    "for i in range(0, columns*rows):\n",
    "    fig.add_subplot(rows, columns, i+1)\n",
    "    if ((i%2)==0):\n",
    "        plt.imshow(w[int(i/2)][0], cmap='gray')\n",
    "    else:\n",
    "        c = cv2.filter2D(img, -1, w[int((i-1)/2)][0])\n",
    "        plt.imshow(c, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: Choose a filter from one of your trained convolutional layers; looking at these activations, what purpose do you think it plays? What kind of feature do you think it detects?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: (does it detect vertical lines or does it blur out noise, etc.) write your answer here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
